{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"MemAlloy","text":"<p>\"The High-Performance Memory Kernel for AI Agents.\"</p> <p>MemAlloy is an open-source \"Memory Kernel\" designed to solve the data ingestion and retrieval bottleneck in Python AI applications. By offloading heavy tasks to a high-performance Rust core, it provides Python developers with a \"Second Brain\" that is 100x faster, memory-efficient, and privacy-first.</p>"},{"location":"#the-problem","title":"The Problem","text":"<p>Python is the language of AI, but it is ill-suited for the infrastructure of AI Memory:</p> <ul> <li>Latency: Watching thousands of files and chunking text introduces massive lag (GIL issues).</li> <li>Complexity: Glueing together <code>watchdog</code>, <code>pypdf</code>, <code>sentence-transformers</code>, <code>chromadb</code>, etc. is fragile.</li> <li>Resource Heaviness: Docker containers for vector DBs consume GBs of RAM.</li> </ul>"},{"location":"#the-solution","title":"The Solution","text":"<p>MemAlloy consolidates the entire RAG pipeline into a single, installable binary that exposes a clean Python API.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\u26a1 Zero-Latency Ingestion: Detects file changes instantly.</li> <li>\ud83e\udde0 Local Intelligence: Runs ONNX embedding models locally on CPU.</li> <li>\ud83d\udcbe Embedded Storage: Uses LanceDB for serverless, persistent vector storage.</li> <li>\ud83d\udc0d Python Native: Simple <code>pip install</code> experience.</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This reference documents the Python API exposed by the <code>memalloy</code> package.</p>"},{"location":"api/#class-ragkernel","title":"Class <code>RAGKernel</code>","text":"<p>The main entry point for the MemAlloy system. It manages the vector database connection, embedding generation, and document retrieval.</p>"},{"location":"api/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, db_path: Optional[str] = None, chunk_size: int = 500, overlap: int = 50)\n</code></pre> <p>Parameters: *   <code>db_path</code> (str, optional):     *   FileSystem path to store the LanceDB database.     *   Default: <code>\"./.memalloy_data\"</code> in the current working directory. *   <code>chunk_size</code> (int, optional):     *   Maximum number of characters per text chunk. Smaller chunks are better for pinpointing specific facts; larger chunks preserve more context.     *   Default: <code>500</code>. *   <code>overlap</code> (int, optional):     *   Number of characters to overlap between adjacent chunks to prevent context loss at boundaries.     *   Default: <code>50</code>.</p> <p>Example: <pre><code>rag = RAGKernel(db_path=\"/tmp/my_knowledge_base\", chunk_size=1000)\n</code></pre></p>"},{"location":"api/#methods","title":"Methods","text":""},{"location":"api/#add_document","title":"<code>add_document</code>","text":"<pre><code>def add_document(self, content: str, metadata: Optional[Dict[str, str]] = None) -&gt; str\n</code></pre> <p>Ingests a single document into the system. The document is immediately chunked, embedded, and saved to disk.</p> <ul> <li>Parameters:<ul> <li><code>content</code> (str): The raw text content of the document.</li> <li><code>metadata</code> (dict, optional): A simple key-value dictionary for storing extra info (e.g., source filename, author, date).</li> </ul> </li> <li>Returns:<ul> <li>(str): A unique UUID generated for this document.</li> </ul> </li> </ul> <p>Example: <pre><code>doc_id = rag.add_document(\n    \"The mitochondria is the powerhouse of the cell.\",\n    metadata={\"subject\": \"biology\"}\n)\n</code></pre></p>"},{"location":"api/#search","title":"<code>search</code>","text":"<pre><code>def search(self, query: str, top_k: int = 5) -&gt; List[Tuple[Document, float]]\n</code></pre> <p>Performs a semantic similarity search against the knowledge base.</p> <ul> <li>Parameters:<ul> <li><code>query</code> (str): The natural language query.</li> <li><code>top_k</code> (int, optional): Number of results to return. Default is 5.</li> </ul> </li> <li>Returns:<ul> <li>A list of tuples, where each tuple is <code>(Document, score)</code>.</li> <li><code>score</code> is a float between 0.0 and 1.0 (approximate), where higher is better.</li> </ul> </li> </ul> <p>Example: <pre><code>results = rag.search(\"energy source cell\")\n</code></pre></p>"},{"location":"api/#count","title":"<code>count</code>","text":"<pre><code>def count(self) -&gt; int\n</code></pre> <ul> <li>Returns: The total number of chunks (vectors) currently stored in the database.</li> </ul>"},{"location":"api/#class-filewatcher","title":"Class <code>FileWatcher</code>","text":"<p>A utility class that wraps a <code>RAGKernel</code> and monitors a filesystem directory for changes.</p>"},{"location":"api/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(self, rag_kernel: RAGKernel, watch_path: str, extensions: Optional[List[str]] = None, recursive: bool = True)\n</code></pre> <p>Parameters: *   <code>rag_kernel</code>: An initialized <code>RAGKernel</code> instance. *   <code>watch_path</code>: Path to the directory to monitor. *   <code>extensions</code>: List of file extensions to include (e.g., <code>[\"txt\", \"md\"]</code>). If None, watches all files. *   <code>recursive</code>: Whether to watch subdirectories. Default <code>True</code>.</p>"},{"location":"api/#methods_1","title":"Methods","text":""},{"location":"api/#sync","title":"<code>sync</code>","text":"<pre><code>def sync(self) -&gt; None\n</code></pre> <p>Performs a one-time scan of the directory and adds any matching files that aren't already in the index (based on path/hash).</p>"},{"location":"api/#start_watching","title":"<code>start_watching</code>","text":"<pre><code>def start_watching(self) -&gt; None\n</code></pre> <p>Spawns a background thread that listens for OS-level file system events (<code>Create</code>, <code>Modify</code>, <code>Delete</code>). Changes are reflected in the RAG Kernel instantly.</p>"},{"location":"api/#stop_watching","title":"<code>stop_watching</code>","text":"<pre><code>def stop_watching(self) -&gt; None\n</code></pre> <p>Stops the background monitoring thread.</p>"},{"location":"api/#class-document","title":"Class <code>Document</code>","text":"<p>A simple data class representing a retrieved result.</p> <p>Properties: *   <code>id</code> (str): The unique ID of the document (or chunk). *   <code>content</code> (str): The text content. *   <code>metadata</code> (dict): The associated metadata.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>MemAlloy is a hybrid system that bridges the ease of Python with the raw performance of Rust. It avoids the common \"Python Glue\" antipattern where Python orchestrates heavy loops, instead pushing the entire data pipeline down into native code.</p>"},{"location":"architecture/#system-diagram","title":"System Diagram","text":""},{"location":"architecture/#component-deep-dive","title":"Component Deep Dive","text":""},{"location":"architecture/#1-the-interface-layer-srclibrs","title":"1. The Interface Layer (<code>src/lib.rs</code>)","text":"<ul> <li>Technology: PyO3</li> <li>Role: Converts Python objects into Rust structs and vice-versa.</li> <li>Optimization: We use <code>PyString</code> and zero-copy references where possible to avoid memory duplication when passing large documents from Python to Rust.</li> </ul>"},{"location":"architecture/#2-the-async-runtime-srcragrs","title":"2. The Async Runtime (<code>src/rag.rs</code>)","text":"<ul> <li>Technology: Tokio</li> <li>Role: Although the Python API is synchronous (blocking for simplicity), the Rust core runs an internal Tokio runtime.</li> <li>Why?: Database I/O (LanceDB) and file system events are inherently asynchronous. We bridge this gap by spawning a temporary runtime for each operation or properly managing a background thread for the <code>FileWatcher</code>.</li> </ul>"},{"location":"architecture/#3-the-embedding-engine-srcembeddingrs","title":"3. The Embedding Engine (<code>src/embedding.rs</code>)","text":"<ul> <li>Technology: FastEmbed-rs + <code>ort</code> (ONNX Runtime)</li> <li>Model: <code>All-MiniLM-L6-v2</code> (Quantized).</li> <li>Performance:<ul> <li>Typical Python implementations use valid PyTorch which is heavy (2GB+ RAM).</li> <li>MemAlloy uses the ONNX runtime which is extremely lightweight (&lt;500MB RAM) and optimized for CPU inference (AVX2/AVX512 instructions).</li> </ul> </li> </ul>"},{"location":"architecture/#4-storage-engine-srcvector_storers","title":"4. Storage Engine (<code>src/vector_store.rs</code>)","text":"<ul> <li>Technology: LanceDB</li> <li>Format: Apache Arrow (Columnar).</li> <li>Schema:<ul> <li><code>id</code>: UUID (String)</li> <li><code>vector</code>: FixedSizeList (384 dimensions) <li><code>document_id</code>: The parent document UUID</li> <li><code>content</code>: The literal text chunk (String)</li> <li><code>metadata</code>: JSON String (for flexible filtering)</li> <li>Benefit: LanceDB allows us to perform Approximate Nearest Neighbor (ANN) search directly on disk without loading the entire index into RAM, making it \"Serverless\" and scalable.</li>"},{"location":"architecture/#concurrency-model","title":"Concurrency Model","text":"<p>When you call <code>rag.add_document()</code>, Python releases the GIL (Global Interpreter Lock) immediately. This allows your Python application (e.g., a FastAPI server or Streamlit app) to remain responsive while Rust crunches the numbers on background threads.</p> <ol> <li>Python call enters Rust.</li> <li>Rust releases GIL.</li> <li>Rust spawns work on rayon/tokio threads.</li> <li>Result computed.</li> <li>Rust re-acquires GIL to return result.</li> </ol>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python: Version 3.8 or higher.</li> <li>Operating System: macOS, Linux, or Windows (WSL2 recommended).</li> <li>Rust Toolchain: Required only if building from source.</li> </ul>"},{"location":"getting_started/#option-1-install-from-source-recommended","title":"Option 1: Install from Source (Recommended)","text":"<p>Since MemAlloy is currently in active development, building from source is the best way to get the latest performance improvements.</p> <pre><code># 1. Install Maturin (The Rust-Python bridge builder)\npip install maturin\n\n# 2. Clone the repository\ngit clone https://github.com/your-username/memalloy\ncd memalloy\n\n# 3. Build and install into your current Python environment\n# --release flag is CRITICAL for performance (10-100x speed difference)\nmaturin develop --release\n</code></pre>"},{"location":"getting_started/#basic-usage","title":"Basic Usage","text":"<p>The core of MemAlloy is the <code>RAGKernel</code>. It handles the entire lifecycle of your data: chunking, embedding, and storage.</p>"},{"location":"getting_started/#1-initialize-the-kernel","title":"1. Initialize the Kernel","text":"<pre><code>from memalloy import RAGKernel\n\n# Initialize with default settings\n# - Creates .memalloy_data folder in current directory\n# - Uses 'AllMiniLML6V2' model (downloads automatically on first run)\nrag = RAGKernel()\n</code></pre>"},{"location":"getting_started/#2-ingest-data-manually","title":"2. Ingest Data manually","text":"<p>You can add raw text strings directly. This is useful for chat logs, API responses, or data you've already parsed.</p> <pre><code># Returns a unique Document ID\ndoc_id = rag.add_document(\n    content=\"Rust's ownership model guarantees memory safety without garbage collection.\",\n    metadata={\"source\": \"The Rust Book\", \"chapter\": 4}\n)\nprint(f\"Ingested Document: {doc_id}\")\n</code></pre>"},{"location":"getting_started/#3-semantic-search","title":"3. Semantic Search","text":"<p>Search your knowledge base using natural language queries.</p> <pre><code>results = rag.search(query=\"Why is Rust safe?\", top_k=3)\n\nfor doc, score in results:\n    print(f\"--- Score: {score:.4f} ---\")\n    print(f\"Content: {doc.content}\")\n    print(f\"Metadata: {doc.metadata}\")\n</code></pre>"},{"location":"getting_started/#advanced-automated-file-watching","title":"Advanced: Automated File Watching","text":"<p>MemAlloy includes a <code>FileWatcher</code> that can monitor a directory and automatically ingest changes in real-time. This is useful for building \"Chat with your Docs\" applications.</p> <pre><code>import time\nfrom memalloy import RAGKernel, FileWatcher\n\n# 1. Setup Kernel\nrag = RAGKernel()\n\n# 2. Setup Watcher\n# Monitors './my_docs' for .md and .txt files\nwatcher = FileWatcher(\n    rag_kernel=rag,\n    watch_path=\"./my_docs\",\n    extensions=[\"md\", \"txt\", \"py\"]\n)\n\n# 3. Initial Sync (Ingest files already there)\nprint(\"Syncing existing files...\")\nwatcher.sync()\n\n# 4. Start Real-time Monitoring (Background Thread)\nprint(\"Watching for changes...\")\nwatcher.start_watching()\n\ntry:\n    # Keep main thread alive\n    while True:\n        time.sleep(1)\nexcept KeyboardInterrupt:\n    watcher.stop_watching()\n    print(\"Stopped.\")\n</code></pre>"},{"location":"getting_started/#configuration","title":"Configuration","text":"Parameter Default Description <code>db_path</code> <code>.memalloy_data</code> Directory where LanceDB stores vectors. <code>chunk_size</code> <code>500</code> Maximum number of characters per chunk. <code>overlap</code> <code>50</code> Number of overlapping characters between chunks. <p>(Note: Embedding model is currently fixed to <code>AllMiniLM-L6-v2</code> for optimal local performance).</p>"}]}